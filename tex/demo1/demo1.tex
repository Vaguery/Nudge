% XeLaTeX can use any Mac OS X font. See the setromanfont command below.
% Input to XeLaTeX is full Unicode, so Unicode characters can be typed directly into the source.

% The next lines tell TeXShop to typeset with xelatex, and to open and save the source with Unicode encoding.

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[12pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper} 
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont[Mapping=tex-text]{LTC Village No.2 Pro}
\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}
\setmonofont[Scale=MatchLowercase]{Andale Mono}

\usepackage{shortvrb}
\usepackage{fancyhdr}
\usepackage{fancyvrb}

\title{Some detailed use cases for the pragmatic GP system}
\author{William Tozier, Vague Innovation LLC}
\date{September 27, 2009}

\begin{document}
\maketitle
\MakeShortVerb{\|}



\section{Symbolic regression}

\textit{Uses: data-driven objectives; built-in functions; multi-objective search; analysis packages}

\subsection{Overview}

Symbolic regression is such a common application of genetic programming techniques that many treat the two as synonymous, though there's a lot more scope to genetic programming than mere data modeling.

Traditional parametric statistical models of systems create a particular (fixed) mathematical relationship between independent ``input'' variables and dependent ``outputs''. Parametric regression methods are searches for the optimal values of the parameters associated with that particular model framework, and generally they aim to minimize some error measure over the correct expected (``correct'') values of outputs and the observed outputs given a vector of parameter values. Linear regression, for instance, assumes there's a linear model of the input--output relations, and finds the set of constant multipliers for the dependent variables that best ``fit'' the observed outputs to a linear surface. Fancy-sounding as it may be, neural network modeling is just another sort of parametric regression approach in which a complicated matrix multiplication is fed through a special kind of nonlinear transform: ``training'' a neural network is just a process of finding the right constant values to multiply things and add them.

In parametric regression the decision maker selects a particular model ``template'' based on first principles or some hypotheses about the relations they expect, fits the parameters of that model to minimize some error measure, and looks at the resulting fit to see if it ``males sense''.

Symbolic regression is a different animal entirely---though even technical folks with passing familiarity assume it's just some kind of variant of parametric regression, and the waters are often muddied because genetic \emph{algorithms} can be used to fit parametric models to data. But where parametric regression is a search for the correct values of the parameters of a given model over particular data, symbolic regression is a search for \emph{a combination of both model and its parameters}. Genetic programming methods wander through much more complex search spaces than mere vectors: they're able to construct and modify the ``templates'' of regression models themselves.

In symbolic regression the decision maker selects a set of ``tools'' or ``parts'' from which models of varying complexity can be constructed, and genetic programming samples the set of possible templates \emph{and} the set of parameters for each model. As with parametric regression, there is a goal of minimizing some error measure for a given model's fit. But there is often also the implicit goal of finding a \emph{parsimonious} model, which is simple enough to be understandable and avoid over-fitting the data.

\subsection{Setting up the problem}

It's safe to assume most symbolic regression problems will want to find ``reasonable sounding'' models based on particular datasets or (in testing and pedagogic situations) known relations. These will have a known, fixed set of named input and output variables, and the desired models will involve more or less standard arithmetic and transcendental functions, with maybe an occasional dollop of conditional logic for ``weird'' models.

\subsubsection{Instructions}

Expect a symbolic regression project to include arithmetic functions like $+$, $-$, $*$, and $\div$; $\sqrt{}$, $\sin{}$, $\log{}$ and $\arctan{}$ will crop up often in some domains, but rarely in others; if there's any expectation that a model structured like this

\begin{equation*}
f(x_{1}, x_{2}, x_{3}) = \left \{ \begin{array}{ll} \frac{c_{1}x_{2}-c_{2}x_{3}}{c_{3}x_{1}x_{2}+c_{4}x_{3}} & \mbox{if $x_{1} \geq c_{6}x_{2}$};\\ -c_{5}\sqrt{x_{3}} & \mbox{if $x_{1} < c_{6}x_{2}$}.\end{array} \right.
\end{equation*}
will ever crop up, some sort of |if...then| statement may need to be included. Beyond the functions familiar from the world of traditional mathematical modeling, it would be surprising to encounter any of the recursion or iteration instructions from the Push3 ``core'' set, like as |code_do| or |exec_k|.

\subsubsection{Types}
Depending on what strikes the modeler as ``reasonable-sounding'', there may be integer or float numbers, with a bit of simple boolean logic to handle conditional expressions.

\subsubsection{Objectives}

These will fall into two broad classes: \emph{error} and \emph{complexity} measures.

Like most supervised learning problems, symbolic regression tries to learn from a user-provided set of examples that are already worked out. Datasets for symbolic regression problems will take the form of a table with columns representing input and output variables, and rows containing individual test cases, which might be subdivided into ``training'' and ``test'' subsets. In ``real world'' applications the data will be measurements; in proof-of-concept and explorations of genetic programming, they may be samples from a known function.

Most error-based objectives will: \begin{enumerate} \item{choose some of the training data rows} \item{for each row selected}\begin{itemize} \item{bind the dependent variables to the appropriate values} \item{run the program} \item{for each dependent variable, sample a value from the final program state} \end{itemize} \item{using the set of expected and observed values for the dependent variables, calculate an \emph{aggregated error measure.}} \end{enumerate} The error measure itself might be a traditional one like summed squared deviations, maximum absolute deviation, root mean squared deviation, or a more domain-specific one. There may also be multiple error objectives, some of which include all the dependent variables, others that include a subset.

Often there are flavors of error crucial to the decision-maker that may not be familiar to many users. For example, in most industrial applications it's crucial to select models for robustness using objectives that look at how quickly the predictive power of a model drops off as errors are added the input variables, or when measurements are missing entirely. There should also be at least one \emph{unselected} testing objective, which should be calculated using the test data instead of the training data, and \emph{should never be the basis of selection during search}. Whether or not these test objectives are identical to one of the training objectives is ultimately a decision for the project owner, but it's a good bet to start with identical measures.

For large-scale problems with hundreds (or millions) of rows in the dataset, it is a waste of computation time to use all the data on the bad-performing individuals from early in the search. Best practices for these large-scale problems involve \emph{vertical slicing} or \emph{} of one sort or another, in which the choice of training examples changes to become more rigorous over the course of a run.

Complexity-based objectives will be aiming to make the models derived more parsimonious, so they might look at the structure of the program, the number of steps it takes, the number of errors it makes during the run, or the size of the stacks during or at the end of the run.

\subsection{Analysis}

Besides watching the time-course of performance on the chosen objectives during a run, there are several basic analyses that a user should run on the models already discovered. They should be able to plot a subset of models on performance planes, for any pair of objectives, and visualize the domination layers. 

Visualizing a single model will at least involve plotting actual vs observed values.

Most sophisticated symbolic regression users will not be looking for emph{the} model for a system, but will want to explore the entire nondominated set in aggregate. One of the most useful analyses is Castillo's exploration of variable transforms: given a large set of high-performing models (of varying complexity), she counted the occurrence of the independent variables in their local context in all the models, finding important ratios and things like log-transforms that ``made sense'' to domain experts when they discussed them.

\subsection{Example}

Suppose the decision-maker is interested in a set of industrial data from a process with 13 inputs (10 floats, 3 integers, one boolean) and four outputs (three floats, one integer). She has a database with 12000 datapoints from the factory. About 100 of the measurements (equally distributed over independent and dependent variables) are missing. After considering the structure of the dataset, the user decides to use the |float|, |int| and |bool| built-in types.

She creates a new training objective function that uses a subsample of 10000 of her dataset rows, and another test objective (identical otherwise) that refers to the remaining 2000 rows. In preparing the training objective, the whole dataset is sorted by the four dependent variable columns (in lexicographic order), and a \emph{vertical slice} made up of every 100th row of the sorted data is created. After every 10000 evaluations, the training is automatically expanded by another 100 samples until the entire set is used.

The test objective uses 10\% of the 2000 test data rows, selected at random every time, to measure test error. The test and training objectives both return the \emph{average absolute error per trial}.

The user also activates two built-in complexity measures: \emph{number of program points} and \emph{total number of items pushed to all stacks during the run}.

She decides to use all these built-in Nudge functions: \begin{itemize} \item{|float_divide|, |float_multiply|, |float_add|, |float_subtract|,} \item{|float_sqrt|, |float_ln|, |float_sine|, |float_exp|,} \item{|int_divide|, |int_multiply|, |int_add|, |int_subtract|, |int_sqrt|,} \item{|float_fromint|, |int_fromfloat|,} \item{|float_if|, |int_if|,} \item{|bool_and|, |bool_or|, |bool_not|,} \item{and the standard stack manipulation instructions for the |bool|, |float| and |int| stacks (|*_dup|, |*_swap|, |*_pop|, |*_flush|).}\end{itemize}

The user starts search using the built-in search daemons and basic parameters, tracing the performance over time new individuals on parallel axes for the four objectives, and occasionally checking a lattice plot of every pair of output variables. The search daemons restart and change their own parameters adaptively over time, so there are periods where improvement is obvious and other times when it seems to be floundering, but there's relatively little call for intervention. Now and then she picks a few individuals out as especially interesting, and the search daemon tries explicitly to refine the parameters of those.

After watching the results for several hundred thousand samples, she realizes her model needs to handle the cases where measurements are missing. She creates a new \emph{robustness objective} function, which samples 100 of the 10000 training rows uniformly, and runs the program using that data \emph{after deleting exactly one of the dependent variables} (chosen with uniform probability). This new objective returns the same error measure as the others.

She now has five objectives, and decides to apply the new objective to the 250000 samples already taken as a low-priority process. While the search daemon keeps producing new samples using the five objectives, a separate evaluation daemon gradually back-fills the performance of the earlier samples by the order of their domination ranks. In other words, it starts with the nondominated individuals, works on the nondominated individuals left after those are removed, and so on. The search speed isn't noticeably reduced with the new process running reniced to a low priority.

After a half-million or so evaluations, she begins to feel she has enough samples to explore variable transforms. She runs another separate analysis package to derive a new dataset that counts the \emph{local context} of each of the 13 input variables. This process looks through all the saved individuals and determines all the program points that use the variables as inputs. So for instance if a particular program is

\begin{Verbatim}[frame=single,numbers=left, xleftmargin=10mm, numbersep=1mm]
block {
  sample :float, 8.91
  channel x3 # float
  channel x12 # float
  instr float_sine
  instr float_multiply
  sample :bool, false
  instr float_add}
\end{Verbatim}
the sampling process will log the fact that the independent variables $x_{12}$ and $x_{3}$ appear, and that $(\sin x_{12})$ and $(x_{3} \sin{x_{12}})$ are their immediate contexts, respectively. From this dataset, she produces a table of the occurrence rates of each of the variables in the best models, and also of the most common contexts, and discovers that several of the variables disappear, and several are transformed most of the time. When she shows these results to domain experts, they recognize most of the transformations from basic textbook models of the physics of the industrial process, but are intrigued by a couple they've never seen before.

By the time she shuts down the search daemons, the decision-maker has a collection of over two million samples and 5000 nondominated models of the process (including many that can handle missing data). The prospects for building industrial controllers, understanding basic principles, improving processes to save money or reduce risk are better now than they were before she started the project.

\section{Image segmentation and classification}

\textit{Uses: data-driven objectives; domain-specific types; multi-objective search; analysis packages; visualization}

\subsection{Overview}


As with symbolic regression, a genetic programming search for something like an image recognizer differs from traditional learning approaches. While a neural or other parametric algorithm can be used to minimize the errors of a particular model, a genetic programming approach is a search for \emph{ways to solve the problem well}: the genetic programming process can handle almost every step from feature identification, modeling, and parametric fitting.

There are many obvious military and industrial applications of image segmentation methods: picking out items from satellite imagery, identifying signs or faces in images, finding regions of text in photos or scanned text, and even disease diagnosis from medical imaging. The main task is another supervised learning problem: given a training set of images with the desired regions identified and classifications assigned, the goal is to discover a method for identifying salient regions and classifications for test images. Segmented images may have polygons superimposed on them, or may produce some sort of pixel-wise mask; images may be classified on multiple bases ranging from ``malignancy'' for medical images to ``similarity'' for search engine applications.

\subsection{Setting up the problem}

\subsubsection{Instructions}

\subsubsection{Types}

\subsubsection{Objectives}

\subsection{Analysis}

\subsection{Example}


\section{OCR preprocessing}

\textit{Uses: unsupervised objectives; domain-specific types; multi-objective search; external simulator}

\subsection{Overview}

Book digitization is a problem. The tens of millions of books Google has scanned and converted into machine-readable text files with optical character recognition (OCR) each have hundreds of pages, and the OCR algorithms are accurate for 19th-century typography at rates as low as 80\% on a per-character basis. That means there are billions of typos in the databases, and plenty of room for improvement.

Rather than trying to reinvent the wheel and search for OCR algorithms as such, in this task we're going to look for \emph{image pre-processing methods} that improve the quality of OCR output. Given a set of several thousand scanned pages for which we have good-quality hand-checked text, we'll search for ways to deskew, blur, sharpen, normalize, denoise, smooth, fill in and otherwise adjust the images to maximize the improvement in an off-the-shelf OCR algorithm's accuracy.


\subsection{Setting up the problem}

\subsubsection{Instructions}

\subsubsection{Types}

\subsubsection{Objectives}

\subsection{Analysis}

\subsection{Example}

\section{Stock picking based on financial datasets}

\textit{Uses: large-scale datasets; domain-specific types; multi-objective search; human-readability constraints; walk-ahead training}

\subsection{Overview}

Almost every newcomer to machine learning, and especially genetic programming, tries his hand at stock market prediction... but most do it wrong. Professional financial managers with substantial portfolios under management are much less interested in predictions of particular stocks' performance than they are the relative performance of \emph{all} equities. As a result, the more interesting applications of genetic programming (for them) are those that examine the entire space of tradable equities, and assign each a scalar rank. The most common use of ranking functions like this is in large-scale high-liquidity portfolios: when a suitable function has been found, the manager can enter a long position in the top-ranked quantile of the set of equities, and a short position in the lowest-ranked quantile.

Professional fund managers have an interesting set of extra constraints, and one that many \emph{pragmatic} genetic programming projects should share: regulatory and institutional requirements demand that the models that are discovered need to \emph{make sense}. In practice, this means that the set of mathematical and logical functions that end up in the best models need to be closely related to common knowledge and tradition: nothing ``weird'' like the cube roots of log prices can be trusted or used. Whether you feel that's rational or not, it presents an interesting challenge for genetic programming projects, which are traditionally open-ended and full of uninterpretable blue-sky solutions.

\subsection{Setting up the problem}

\subsubsection{Instructions}

\subsubsection{Types}

\subsubsection{Objectives}

\subsection{Analysis}

\subsection{Example}

\section{Playing the peg solitaire game}

\textit{Uses: external simulator; interactive instructions; domain-specific types; multi-objective search}

\subsection{Overview}

\subsection{Setting up the problem}

\subsubsection{Instructions}

\subsubsection{Types}

\subsubsection{Objectives}

\subsection{Analysis}

\subsection{Example}

\section{Searching for linear programming algorithms}

\textit{Uses: unsupervised learning; coevolution of training cases; domain-specific types; multi-objective search}

\subsection{Overview}

\subsection{Setting up the problem}

\subsubsection{Instructions}

\subsubsection{Types}

\subsubsection{Objectives}

\subsection{Analysis}

\subsection{Example}

\end{document}  